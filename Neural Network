import numpy
%pylab inline --no-import-all
with np.load('training-dataset.npz')  as data:
    img = data['x']
    lbl = data['y']
for im in range(10):
    image = img[im]
    image = np.array(image, dtype='float')
    pixels = image.reshape((28, 28))
    plt.imshow(pixels)
    plt.show()
from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(img, lbl, test_size=1/3, random_state=999)
print(X_train.shape)
print(y_train.shape)
from sklearn.preprocessing import LabelBinarizer
onehot = LabelBinarizer()
Y_train = onehot.fit_transform(y_train)
Y_val   = onehot.transform(y_val)
print(Y_train.shape)
print((np.unique(lbl)))
# Normalize the images.
X_train = (X_train / 255) - 0.5
X_val = (X_val / 255) - 0.5

# Flatten the images.
X_train = X_train.reshape((-1, 784))
X_val = X_val.reshape((-1, 784))

print(X_train.shape) # (60000, 784)
print(X_val.shape)  # (10000, 784)
## Neural nets
#.....................................
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import Adam, SGD

model = Sequential()
model.add(Dense(76, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(76, activation='relu'))
model.add(Dense(Y_train.shape[1], activation='softmax')) # We need to have as many units as classes, 
                                                             # and softmax activation
optimizer = Adam(lr=0.0001)
# For classification, the loss function should be categorical_crossentropy
model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy'])
#Train the model
model.fit(X_train, Y_train, epochs=10, batch_size=16, verbose=0 )
from sklearn.metrics import accuracy_score
y_pred = model.predict_classes(X_val, verbose=0)
print(accuracy_score(y_val, y_pred))
