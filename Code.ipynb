{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Machine Learning Challenge: Image Classification</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goutham Deekshit Indiran  | u195004 <br>\n",
    "Manav Mishra | u558101 <br>\n",
    "Sadjia Safdari | u265740"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%pylab inline --no-import-all\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "## Neural nets\n",
    "#.....................................\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, GaussianNoise\n",
    "from keras.callbacks import History\n",
    "from keras.metrics import TopKCategoricalAccuracy, SparseTopKCategoricalAccuracy\n",
    "\n",
    "## Image processing packages\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import itertools\n",
    "from numpy import asarray\n",
    "\n",
    "## Matplot lib\n",
    "#.....................................\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random Forest\n",
    "import time\n",
    "import random\n",
    "from sklearn import ensemble \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"data/training-dataset.npz\") as data:\n",
    "        img = data[\"x\"] # 97843200\n",
    "        lbl = data[\"y\"] # 124800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.shape)\n",
    "\n",
    "for im in range(10):\n",
    "    image = img[im]\n",
    "    image = np.array(image, dtype='float')\n",
    "    pixels = image.reshape((28, 28))\n",
    "    plt.imshow(pixels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and validation +test with 80% for training data \n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(img, lbl, test_size=0.2,random_state=1) \n",
    "\n",
    "# Splitting validation + test \n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.3, random_state=1)\n",
    "\n",
    "# Visualising the shape of the train and test data\n",
    "print(X_train.shape) \n",
    "print(X_test.shape) \n",
    "print(X_val.shape) \n",
    "print(y_test.shape) \n",
    "print(y_val.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data so that it runs faster during fitting the model\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "y_val = y_val.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot decoding \n",
    "onehot = LabelBinarizer() # transform categorical target to dummies to train the NN\n",
    "Y_train = onehot.fit_transform(y_train)\n",
    "Y_val   = onehot.fit_transform(y_val)\n",
    "Y_test   = onehot.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fitting the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fitting the compiling and fitting the sequential model\n",
    "#.....................................\n",
    "\n",
    "es = EarlyStopping(monitor=\"val_loss\")\n",
    "\n",
    "model = Sequential() # initiates model\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(350, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax')) # We need to have as many units as classes, and softmax activation\n",
    "\n",
    "# define parameters for training of the model    \n",
    "optimizer = Adam(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = ['accuracy']) # for classification, the loss function should be categorical_crossentropy\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=450, validation_data=(X_val, Y_val), verbose=1, callbacks = [es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy and Loss visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "\n",
    "accuracy = history.history['accuracy']\n",
    "#summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicting the classes on test set and printing the accuracy\n",
    "y_pred = model.predict_classes(X_test, verbose=1)\n",
    "print(accuracy_score(y_test, y_pred)) # 0.889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(X_test,Y_test)\n",
    "# Model summary\n",
    "model.summary()\n",
    "print((y_pred[:10]))\n",
    "print((y_test[:10]).astype('int32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Random Forest classifier\n",
    "#oob_score This is a random forest cross validation method \n",
    "#This comes out very handy while scalling up a particular function from prototype to final dataset.\n",
    "n_estimators = np.arange(100,1100,100)\n",
    "t0 = time.time()\n",
    "for i in n_estimators: \n",
    "    classifier = ensemble.RandomForestClassifier(n_estimators = i, oob_score = True, n_jobs = -1,random_state =50)\n",
    "    classifier.fit(X_train,y_train)\n",
    "    # evaluating the model on the validation set and checking best value of k\n",
    "    score = classifier.score(X_val,y_val)\n",
    "    print(\"No. of trees = %d, accuracy=%.2f%%\" % (i, score * 100))\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model on train data with best value of n_estimators\n",
    "classifier = ensemble.RandomForestClassifier(n_estimators = 1000, oob_score = True, n_jobs = -1,random_state =50)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "score = classifier.score(X_test,y_test) \n",
    "score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Task 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Adding Gaussian noise to CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling and fitting the sequential model\n",
    "#.....................................\n",
    "\n",
    "es = EarlyStopping(monitor= 'val_loss')\n",
    "optimizer = Adam(lr=0.0001)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding = 'same',strides=(1, 1),activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(GaussianNoise(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax')) # We need to have as many units as classes, and softmax activation\n",
    "\n",
    "# For classification, the loss function should be categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n",
    "history1 = model.fit(X_train, Y_train, epochs=100, batch_size = 450, validation_data = (X_val, Y_val), verbose=1, callbacks = [es])\n",
    "\n",
    "# accuracy of 0.9253 > 23/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()\n",
    "y_pred1 = model.predict_classes(X_test, verbose=1)\n",
    "print(accuracy_score(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict_classes(X_test, verbose=1)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.load('test-dataset.npy')\n",
    "print(img_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new folder\n",
    "cwd = os.getcwd()\n",
    "new_folder = \"test_dataset_images\"\n",
    "folder = os.path.join(cwd, new_folder)\n",
    "os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_data)):\n",
    "    data = Image.fromarray(img_data[i])\n",
    "    if data.mode != 'RGB':\n",
    "        data = data.convert('RGB')\n",
    "    data.save(str(folder)+'\\\\test_image_'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting image corrdinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(get_value):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(coord_list):\n",
    "    new_coord=[]\n",
    "    updated_coord=[]\n",
    "    final_coord=[]\n",
    "    for i in range(len(coord_list)):\n",
    "            args = [iter(coord_list)] * 4\n",
    "            new_coord = list(itertools.zip_longest(*args, fillvalue=None))\n",
    "    return sorted(new_coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"test_dataset_images\"\n",
    "prediction_list = []\n",
    "for filename in os.listdir(folder):\n",
    "    coord_list=[]\n",
    "    new_image_list=[]\n",
    "    image_pred=[]\n",
    "    sample_image = cv2.imread(os.path.join(folder,filename))\n",
    "    if sample_image is not None:\n",
    "        median_blr = cv2.medianBlur(sample_image, 3)\n",
    "        plt.imshow(median_blr)\n",
    "        median_blr.shape\n",
    "        copy = median_blr.copy()\n",
    "        gray = cv2.cvtColor(median_blr, cv2.COLOR_BGR2GRAY)\n",
    "        prediction_image = gray\n",
    "        thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "        ROI_number = 0\n",
    "        cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "        for c in cnts:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            ROI = img[y:y+h, x:x+w]\n",
    "            if w > 9  or h > 10 :\n",
    "                coord_list.append(x)\n",
    "                coord_list.append(w)\n",
    "                coord_list.append(y)\n",
    "                coord_list.append(h)\n",
    "            cv2.rectangle(copy,(x,y),(x+w,y+h),(36,255,12),1)\n",
    "            ROI_number += 1\n",
    "    imk=prediction_image\n",
    "    coords= split_image(coord_list)\n",
    "    plt.imshow(copy)\n",
    "    plt.show()\n",
    "    for i in range(len(coords)):\n",
    "        if coords[i][1] >= 30 :\n",
    "            new_w = coords[i][1] //2\n",
    "            x_temp = 0\n",
    "            for j in range(2):\n",
    "                w = new_w\n",
    "                imtest1=imk[coords[i][2]:coords[i][2]+coords[i][3],coords[i][0] + x_temp: coords[i][0] + x_temp + new_w]\n",
    "                res1 = cv2.resize(imtest1, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "                new_image_list.append(np.asarray(res1))\n",
    "                image_pred=np.array(new_image_list)\n",
    "                image_pred = image_pred.reshape(image_pred.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "                preds1=model.predict_classes(image_pred, verbose=1)\n",
    "                probs = model.predict_proba(image_pred)\n",
    "                best_n = np.argsort(probs, axis=1)[:,-5:]\n",
    "                best_transpose = np.transpose(best_n+1)\n",
    "                best_n_str = np.char.zfill(best_transpose.astype(str), 2)\n",
    "                join_best_n = ','.join([''.join(row) for row in best_n_str])\n",
    "                print(i, len(coords))\n",
    "                if i == len(coords)-1:\n",
    "                    prediction_list.append([join_best_n])\n",
    "                pred_label = preds1[:len(coords)+1]+1\n",
    "                del coord_list[:]\n",
    "                np.delete(image_pred,0,0)\n",
    "                np.delete(best_n,0,0)\n",
    "                x_temp = new_w\n",
    "        else:\n",
    "            imtest1=imk[coords[i][2]:coords[i][2]+coords[i][3],coords[i][0]:coords[i][0]+coords[i][1]]\n",
    "            res1 = cv2.resize(imtest1, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "            new_image_list.append(np.asarray(res1))\n",
    "            image_pred=np.array(new_image_list)\n",
    "            image_pred = image_pred.reshape(image_pred.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "            preds1=model.predict_classes(image_pred, verbose=1)\n",
    "            probs = model.predict_proba(image_pred)\n",
    "            best_n = np.argsort(probs, axis=1)[:,-5:]\n",
    "            best_transpose = np.transpose(best_n+1)\n",
    "            best_n_str = np.char.zfill(best_transpose.astype(str), 2)\n",
    "            join_best_n = ','.join([''.join(row) for row in best_n_str])\n",
    "            print(i, len(coords))\n",
    "            if i == len(coords)-1:\n",
    "                prediction_list.append([join_best_n])\n",
    "            pred_label = preds1[:len(coords)+1]+1\n",
    "            del coord_list[:]\n",
    "            np.delete(image_pred,0,0)\n",
    "            np.delete(best_n,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save predictions into csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('prediction.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in prediction_list:\n",
    "        for in_item in item:\n",
    "            it = in_item.strip().split(',')\n",
    "            writer.writerow(it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
