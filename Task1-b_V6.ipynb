{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%pylab inline --no-import-all\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "## Neural nets\n",
    "#.....................................\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, GaussianNoise\n",
    "from keras.callbacks import History\n",
    "from PIL import Image\n",
    "from keras.metrics import TopKCategoricalAccuracy, SparseTopKCategoricalAccuracy\n",
    "import np_utils\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import itertools\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load('training-dataset.npz')  as data:\n",
    "    img = data['x']\n",
    "    lbl = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test with 80% for training data and 10% for test data\n",
    "\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(img, lbl, test_size=0.2,random_state=1) \n",
    "\n",
    "# Splitting the train data again into validation and train data with 80% of the train data for training and the \n",
    "# remaining 10% for validation purpose\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.3, random_state=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data so that it runs faster during fitting the model\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "y_val = y_val.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot decoding \n",
    "onehot = LabelBinarizer()\n",
    "Y_train = onehot.fit_transform(y_train)\n",
    "Y_val   = onehot.fit_transform(y_val)\n",
    "Y_test   = onehot.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor= 'val_loss')\n",
    "optimizer = Adam(lr=0.0001)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), strides=(1, 1),activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(350, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax')) # We need to have as many units as classes, and softmax activation\n",
    "\n",
    "# For classification, the loss function should be categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n",
    "history = model.fit(X_train, Y_train, batch_size = 450, epochs=100, validation_data = (X_val, Y_val), verbose=1, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk.\n",
    "#model.save_weights('model.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('model.h5')\n",
    "\n",
    "# Predicting the classes on test set and printing the accuracy\n",
    "y_pred = model.predict_classes(X_test, verbose=1)\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model\n",
    "model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 \n",
    "# Appeding leading zero to label\n",
    "# Making the model Robust against noises in the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot decoding \n",
    "onehot = LabelBinarizer()\n",
    "Y_train = onehot.fit_transform(y_train)\n",
    "Y_val   = onehot.fit_transform(y_val)\n",
    "Y_test   = onehot.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling and fitting the sequential model\n",
    "#.....................................\n",
    "es = EarlyStopping(monitor= 'val_loss')\n",
    "optimizer = Adam(lr=0.0001)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), padding = 'same',strides=(1, 1),activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(GaussianNoise(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(360, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(Y_train.shape[1], activation='softmax')) # We need to have as many units as classes, and softmax activation\n",
    "\n",
    "# For classification, the loss function should be categorical_crossentropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics = ['accuracy'])\n",
    "history1 = model.fit(X_train, Y_train, epochs=100, batch_size = 450, validation_data = (X_val, Y_val), verbose=1, callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summarize history for accuracy\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.plot(history1.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='lower right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "\n",
    "plt.plot(history1.history['loss'])\n",
    "plt.plot(history1.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict_classes(X_test, verbose=1)\n",
    "print(accuracy_score(y_test, y_pred1))\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = np.load('test-dataset.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new folder\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "new_folder = \"test_dataset_images\"\n",
    "folder = os.path.join(cwd, new_folder)\n",
    "os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting array to images and saving it in current work directory\n",
    "for i in range(25):\n",
    "    data = Image.fromarray(img_data[i])\n",
    "    if data.mode != 'RGB':\n",
    "        data = data.convert('RGB')\n",
    "    data.save(str(folder)+'\\\\test_image_'+str(i)+'.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(get_value):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(coord_list):\n",
    "    new_coord=[]\n",
    "    updated_coord=[]\n",
    "    final_coord=[]\n",
    "    for i in range(len(coord_list)):\n",
    "            args = [iter(coord_list)] * 4\n",
    "            new_coord = list(itertools.zip_longest(*args, fillvalue=None))\n",
    "    return sorted(new_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=\"test_dataset_images\"\n",
    "prediction_list = []\n",
    "for filename in os.listdir(folder):\n",
    "    coord_list=[]\n",
    "    new_image_list=[]\n",
    "    image_pred=[]\n",
    "    sample_image = cv2.imread(os.path.join(folder,filename))\n",
    "    if sample_image is not None:\n",
    "        median_blr = cv2.medianBlur(sample_image, 3)\n",
    "        plt.imshow(median_blr)\n",
    "        median_blr.shape\n",
    "        copy = median_blr.copy()\n",
    "        gray = cv2.cvtColor(median_blr, cv2.COLOR_BGR2GRAY)\n",
    "        prediction_image = gray\n",
    "        thresh = cv2.threshold(gray,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "        ROI_number = 0\n",
    "        cnts = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "        for c in cnts:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            ROI = img[y:y+h, x:x+w]\n",
    "            if w > 9  or h > 10 :\n",
    "                coord_list.append(x)\n",
    "                coord_list.append(w)\n",
    "                coord_list.append(y)\n",
    "                coord_list.append(h)\n",
    "            cv2.rectangle(copy,(x,y),(x+w,y+h),(36,255,12),1)\n",
    "            ROI_number += 1\n",
    "    imk=prediction_image\n",
    "    coords= split_image(coord_list)\n",
    "    for i in range(len(coords)):\n",
    "        if coords[i][1] >= 30 :\n",
    "            new_w = coords[i][1] //2\n",
    "            x_temp = 0\n",
    "            for j in range(2):\n",
    "                w = new_w\n",
    "                imtest1=imk[coords[i][2]:coords[i][2]+coords[i][3],coords[i][0] + x_temp: coords[i][0] + x_temp + new_w]\n",
    "                res1 = cv2.resize(imtest1, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "                new_image_list.append(np.asarray(res1))\n",
    "                image_pred=np.array(new_image_list)\n",
    "                image_pred = image_pred.reshape(image_pred.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "                preds1=model.predict_classes(image_pred, verbose=1)\n",
    "                probs = model.predict_proba(image_pred)\n",
    "                best_n = np.argsort(probs, axis=1)[:,-5:]\n",
    "                best_transpose = np.transpose(best_n+1)\n",
    "                best_n_str = np.char.zfill(best_transpose.astype(str), 2)\n",
    "                join_best_n = ','.join([''.join(row) for row in best_n_str])\n",
    "                print(i, len(coords))\n",
    "                if i == len(coords)-1:\n",
    "                    prediction_list.append([join_best_n])\n",
    "                pred_label = preds1[:len(coords)+1]+1\n",
    "                del coord_list[:]\n",
    "                np.delete(image_pred,0,0)\n",
    "                np.delete(best_n,0,0)\n",
    "                x_temp = new_w\n",
    "        else:\n",
    "            imtest1=imk[coords[i][2]:coords[i][2]+coords[i][3],coords[i][0]:coords[i][0]+coords[i][1]]\n",
    "            res1 = cv2.resize(imtest1, dsize=(28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "            new_image_list.append(np.asarray(res1))\n",
    "            image_pred=np.array(new_image_list)\n",
    "            image_pred = image_pred.reshape(image_pred.shape[0], 28, 28, 1).astype(\"float32\") / 255\n",
    "            preds1=model.predict_classes(image_pred, verbose=1)\n",
    "            probs = model.predict_proba(image_pred)\n",
    "            best_n = np.argsort(probs, axis=1)[:,-5:]\n",
    "            best_transpose = np.transpose(best_n+1)\n",
    "            best_n_str = np.char.zfill(best_transpose.astype(str), 2)\n",
    "            join_best_n = ','.join([''.join(row) for row in best_n_str])\n",
    "            print(i, len(coords))\n",
    "            if i == len(coords)-1:\n",
    "                prediction_list.append([join_best_n])\n",
    "            pred_label = preds1[:len(coords)+1]+1\n",
    "            del coord_list[:]\n",
    "            np.delete(image_pred,0,0)\n",
    "            np.delete(best_n,0,0)\n",
    "            \n",
    "print(\"Final Prediction List:\", prediction_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('prediction.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    for item in prediction_list:\n",
    "        for in_item in item:\n",
    "            it = in_item.strip().split(',')\n",
    "            writer.writerow(it)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
