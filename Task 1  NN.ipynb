{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import tensorflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumpy version = 1.19.3\\nsklearn version = 0.24\\ntensorflow version = 2.4.0\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "numpy version = 1.19.3\n",
    "sklearn version = 0.24\n",
    "tensorflow version = 2.4.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(\"training-dataset.npz\") as data:\n",
    "    img = data[\"x\"]\n",
    "    lbl = data['y'] \n",
    "\n",
    "lbl = lbl - 1 # the label data starts at 1 where our predict_classes will start at 0, therefore I changed lbl in order to match with the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22  6 15 ... 12 14 18]\n"
     ]
    }
   ],
   "source": [
    "print(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_alt, y_train_unbin, y_alt = train_test_split(img, lbl, test_size=3/10) #documentation: https://www.researchgate.net/post/Is-there-an-ideal-ratio-between-a-training-set-and-validation-set-Which-trade-off-would-you-suggest\n",
    "X_val, X_test, y_val_unbin, y_test = train_test_split(X_alt, y_alt, test_size=1/2, random_state=456) #data is split twice as we need train, val, test\n",
    "\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train_unbin) #dummy codes are needed to train the NN\n",
    "y_val = lb.transform(y_val_unbin)\n",
    "\n",
    "X_train_val = np.vstack((X_train, X_val))\n",
    "y_train_val = np.vstack((y_train, y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18720,)\n",
      "(37440, 784)\n",
      "(87360,)\n",
      "(18720, 784)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(X_alt.shape)\n",
    "print(y_train_unbin.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747542735042735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manavmishra/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "### Calculate the baseline accuracy, ours should be higher\n",
    "\n",
    "baseline = LogisticRegression()\n",
    "baseline.fit(X_alt, y_alt)\n",
    "print(accuracy_score(y_test, baseline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "11000/87360 [==>...........................] - ETA: 1:37 - loss: 2.1393 - acc: 0.3996- ETA: 1:39 - loss: 2.2"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-277d9346a92b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Accuracy and categorical cross-entropy is used as it's a classification problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0my_pred_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Predict_classes raises an error and cannot be used soon, thus we use this instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "activation = 'tanh'\n",
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "epochs = 300\n",
    "Ni = img.shape[1]\n",
    "No = 26\n",
    "Ns = y_train.size\n",
    "alpha = 5\n",
    "beta = [0.9, 0.8]\n",
    "layer_size = Ns / (alpha * (Ni + No))\n",
    "\n",
    "\n",
    "model = Sequential() #Initiates the model\n",
    "model.add(Dense(Ni, input_dim=img.shape[1], activation=activation)) #This is our input layer, has the same number of nodes as dimensions in our data\n",
    "model.add(Dense(layer_size, activation=activation)) # This is the hidden layers\n",
    "model.add(Dense(No, activation='softmax'))#Output layer. We have 26 letters, so output layers should have 26 units. Softmax is used for multi-class classification\n",
    "                                                            \n",
    "optimizer = Adam(lr=learning_rate, beta_1=beta[0], beta_2=beta[1])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = [\"accuracy\"]) #Accuracy and categorical cross-entropy is used as it's a classification problem\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=batch_size, verbose=1)\n",
    "\n",
    "y_pred_train = np.argmax(model.predict(X_train), axis=-1) #Predict_classes raises an error and cannot be used soon, thus we use this instead\n",
    "acc_train = round(accuracy_score(y_train_unbin, y_pred_train), 3) #We calculate the accuracy on train, test and val set and print them at the end. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560.8296296296296\n"
     ]
    }
   ],
   "source": [
    "print(layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.968\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = np.argmax(model.predict(X_train), axis=-1) #Predict_classes raises an error and cannot be used soon, thus we use this instead\n",
    "acc_train = round(accuracy_score(y_train_unbin, y_pred_train), 3)\n",
    "print(acc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "106080/106080 [==============================] - 37s 353us/sample - loss: 1.1257 - acc: 0.6777\n",
      "Epoch 2/100\n",
      "106080/106080 [==============================] - 45s 426us/sample - loss: 0.7016 - acc: 0.7946\n",
      "Epoch 3/100\n",
      "106080/106080 [==============================] - 44s 418us/sample - loss: 0.5731 - acc: 0.8289\n",
      "Epoch 4/100\n",
      "106080/106080 [==============================] - 42s 400us/sample - loss: 0.5011 - acc: 0.8488\n",
      "Epoch 5/100\n",
      "106080/106080 [==============================] - 49s 457us/sample - loss: 0.4484 - acc: 0.8632\n",
      "Epoch 6/100\n",
      "106080/106080 [==============================] - 46s 436us/sample - loss: 0.4139 - acc: 0.8723\n",
      "Epoch 7/100\n",
      "106080/106080 [==============================] - 49s 458us/sample - loss: 0.3808 - acc: 0.8822\n",
      "Epoch 8/100\n",
      "106080/106080 [==============================] - 46s 431us/sample - loss: 0.3595 - acc: 0.8867\n",
      "Epoch 9/100\n",
      "106080/106080 [==============================] - 47s 440us/sample - loss: 0.3440 - acc: 0.8923\n",
      "Epoch 10/100\n",
      "106080/106080 [==============================] - 46s 433us/sample - loss: 0.3313 - acc: 0.8952\n",
      "Epoch 11/100\n",
      "106080/106080 [==============================] - 51s 480us/sample - loss: 0.3162 - acc: 0.8997\n",
      "Epoch 12/100\n",
      "106080/106080 [==============================] - 45s 424us/sample - loss: 0.3059 - acc: 0.9026\n",
      "Epoch 13/100\n",
      "106080/106080 [==============================] - 45s 425us/sample - loss: 0.2958 - acc: 0.9054\n",
      "Epoch 14/100\n",
      "106080/106080 [==============================] - 49s 458us/sample - loss: 0.2850 - acc: 0.9085\n",
      "Epoch 15/100\n",
      "106080/106080 [==============================] - 45s 420us/sample - loss: 0.2762 - acc: 0.9106\n",
      "Epoch 16/100\n",
      "106080/106080 [==============================] - 47s 439us/sample - loss: 0.2721 - acc: 0.9120\n",
      "Epoch 17/100\n",
      "106080/106080 [==============================] - 48s 457us/sample - loss: 0.2651 - acc: 0.9135\n",
      "Epoch 18/100\n",
      "106080/106080 [==============================] - 47s 447us/sample - loss: 0.2593 - acc: 0.9158\n",
      "Epoch 19/100\n",
      "106080/106080 [==============================] - 48s 457us/sample - loss: 0.2528 - acc: 0.9166\n",
      "Epoch 20/100\n",
      "106080/106080 [==============================] - 62s 581us/sample - loss: 0.2495 - acc: 0.9191\n",
      "Epoch 21/100\n",
      "106080/106080 [==============================] - 46s 435us/sample - loss: 0.2414 - acc: 0.9202\n",
      "Epoch 22/100\n",
      "106080/106080 [==============================] - 46s 430us/sample - loss: 0.2377 - acc: 0.9211\n",
      "Epoch 23/100\n",
      "106080/106080 [==============================] - 42s 392us/sample - loss: 0.2336 - acc: 0.9225\n",
      "Epoch 24/100\n",
      "106080/106080 [==============================] - 44s 411us/sample - loss: 0.2311 - acc: 0.9231\n",
      "Epoch 25/100\n",
      "106080/106080 [==============================] - 41s 382us/sample - loss: 0.2262 - acc: 0.9252\n",
      "Epoch 26/100\n",
      "106080/106080 [==============================] - 42s 394us/sample - loss: 0.2190 - acc: 0.9271\n",
      "Epoch 27/100\n",
      "106080/106080 [==============================] - 45s 427us/sample - loss: 0.2149 - acc: 0.9288\n",
      "Epoch 28/100\n",
      "106080/106080 [==============================] - 40s 378us/sample - loss: 0.2125 - acc: 0.9291\n",
      "Epoch 29/100\n",
      "106080/106080 [==============================] - 46s 433us/sample - loss: 0.2107 - acc: 0.9285\n",
      "Epoch 30/100\n",
      "106080/106080 [==============================] - 44s 419us/sample - loss: 0.2053 - acc: 0.9317\n",
      "Epoch 31/100\n",
      "106080/106080 [==============================] - 53s 498us/sample - loss: 0.2024 - acc: 0.9321\n",
      "Epoch 32/100\n",
      "106080/106080 [==============================] - 55s 522us/sample - loss: 0.2004 - acc: 0.9326\n",
      "Epoch 33/100\n",
      "106080/106080 [==============================] - 57s 536us/sample - loss: 0.1975 - acc: 0.9335\n",
      "Epoch 34/100\n",
      "106080/106080 [==============================] - 51s 477us/sample - loss: 0.1971 - acc: 0.9340\n",
      "Epoch 35/100\n",
      "106080/106080 [==============================] - 53s 497us/sample - loss: 0.1934 - acc: 0.9352\n",
      "Epoch 36/100\n",
      "106080/106080 [==============================] - 48s 452us/sample - loss: 0.1881 - acc: 0.9366\n",
      "Epoch 37/100\n",
      "106080/106080 [==============================] - 51s 482us/sample - loss: 0.1895 - acc: 0.9361\n",
      "Epoch 38/100\n",
      "106080/106080 [==============================] - 66s 620us/sample - loss: 0.1844 - acc: 0.9375\n",
      "Epoch 39/100\n",
      "106080/106080 [==============================] - 65s 610us/sample - loss: 0.1826 - acc: 0.9379\n",
      "Epoch 40/100\n",
      "106080/106080 [==============================] - 60s 569us/sample - loss: 0.1816 - acc: 0.9385\n",
      "Epoch 41/100\n",
      "106080/106080 [==============================] - 61s 573us/sample - loss: 0.1793 - acc: 0.9383\n",
      "Epoch 42/100\n",
      "106080/106080 [==============================] - 65s 612us/sample - loss: 0.1761 - acc: 0.9400\n",
      "Epoch 43/100\n",
      "106080/106080 [==============================] - 50s 472us/sample - loss: 0.1733 - acc: 0.9409\n",
      "Epoch 44/100\n",
      "106080/106080 [==============================] - 45s 423us/sample - loss: 0.1729 - acc: 0.9407\n",
      "Epoch 45/100\n",
      "106080/106080 [==============================] - 44s 417us/sample - loss: 0.1704 - acc: 0.9418\n",
      "Epoch 46/100\n",
      "106080/106080 [==============================] - 47s 440us/sample - loss: 0.1699 - acc: 0.9416\n",
      "Epoch 47/100\n",
      "106080/106080 [==============================] - 45s 423us/sample - loss: 0.1660 - acc: 0.9425\n",
      "Epoch 48/100\n",
      "106080/106080 [==============================] - 44s 414us/sample - loss: 0.1649 - acc: 0.9437\n",
      "Epoch 49/100\n",
      "106080/106080 [==============================] - 44s 412us/sample - loss: 0.1661 - acc: 0.9433\n",
      "Epoch 50/100\n",
      "106080/106080 [==============================] - 46s 430us/sample - loss: 0.1646 - acc: 0.9433\n",
      "Epoch 51/100\n",
      "106080/106080 [==============================] - 45s 427us/sample - loss: 0.1631 - acc: 0.9439\n",
      "Epoch 52/100\n",
      "106080/106080 [==============================] - 45s 426us/sample - loss: 0.1586 - acc: 0.9454\n",
      "Epoch 53/100\n",
      "106080/106080 [==============================] - 51s 479us/sample - loss: 0.1572 - acc: 0.9455\n",
      "Epoch 54/100\n",
      "106080/106080 [==============================] - 46s 436us/sample - loss: 0.1577 - acc: 0.9459\n",
      "Epoch 55/100\n",
      "106080/106080 [==============================] - 45s 425us/sample - loss: 0.1541 - acc: 0.9466\n",
      "Epoch 56/100\n",
      "106080/106080 [==============================] - 42s 393us/sample - loss: 0.1543 - acc: 0.9472\n",
      "Epoch 57/100\n",
      "106080/106080 [==============================] - 44s 418us/sample - loss: 0.1523 - acc: 0.9472\n",
      "Epoch 58/100\n",
      "106080/106080 [==============================] - 44s 413us/sample - loss: 0.1517 - acc: 0.9474\n",
      "Epoch 59/100\n",
      "106080/106080 [==============================] - 44s 413us/sample - loss: 0.1517 - acc: 0.9473\n",
      "Epoch 60/100\n",
      "106080/106080 [==============================] - 45s 422us/sample - loss: 0.1508 - acc: 0.9479\n",
      "Epoch 61/100\n",
      "106080/106080 [==============================] - 44s 413us/sample - loss: 0.1469 - acc: 0.9490\n",
      "Epoch 62/100\n",
      "106080/106080 [==============================] - 46s 432us/sample - loss: 0.1465 - acc: 0.9494\n",
      "Epoch 63/100\n",
      "106080/106080 [==============================] - 46s 431us/sample - loss: 0.1454 - acc: 0.9488\n",
      "Epoch 64/100\n",
      "106080/106080 [==============================] - 47s 439us/sample - loss: 0.1438 - acc: 0.9501\n",
      "Epoch 65/100\n",
      "106080/106080 [==============================] - 46s 437us/sample - loss: 0.1421 - acc: 0.9504\n",
      "Epoch 66/100\n",
      "106080/106080 [==============================] - 47s 438us/sample - loss: 0.1389 - acc: 0.9512\n",
      "Epoch 67/100\n",
      "106080/106080 [==============================] - 44s 418us/sample - loss: 0.1386 - acc: 0.9514\n",
      "Epoch 68/100\n",
      "106080/106080 [==============================] - 44s 412us/sample - loss: 0.1381 - acc: 0.9510\n",
      "Epoch 69/100\n",
      "106080/106080 [==============================] - 44s 412us/sample - loss: 0.1353 - acc: 0.9525\n",
      "Epoch 70/100\n",
      "106080/106080 [==============================] - 44s 417us/sample - loss: 0.1337 - acc: 0.9533\n",
      "Epoch 71/100\n",
      "106080/106080 [==============================] - 44s 415us/sample - loss: 0.1310 - acc: 0.9540\n",
      "Epoch 72/100\n",
      "106080/106080 [==============================] - 45s 424us/sample - loss: 0.1302 - acc: 0.9546\n",
      "Epoch 73/100\n",
      "106080/106080 [==============================] - 46s 438us/sample - loss: 0.1317 - acc: 0.9534\n",
      "Epoch 74/100\n",
      "106080/106080 [==============================] - 45s 421us/sample - loss: 0.1301 - acc: 0.9545\n",
      "Epoch 75/100\n",
      "106080/106080 [==============================] - 44s 418us/sample - loss: 0.1310 - acc: 0.9535\n",
      "Epoch 76/100\n",
      "106080/106080 [==============================] - 46s 430us/sample - loss: 0.1282 - acc: 0.9546\n",
      "Epoch 77/100\n",
      "106080/106080 [==============================] - 45s 423us/sample - loss: 0.1242 - acc: 0.9562\n",
      "Epoch 78/100\n",
      "106080/106080 [==============================] - 46s 432us/sample - loss: 0.1262 - acc: 0.9547\n",
      "Epoch 79/100\n",
      "106080/106080 [==============================] - 48s 448us/sample - loss: 0.1235 - acc: 0.9562\n",
      "Epoch 80/100\n",
      "106080/106080 [==============================] - 46s 435us/sample - loss: 0.1233 - acc: 0.9556\n",
      "Epoch 81/100\n",
      "106080/106080 [==============================] - 44s 418us/sample - loss: 0.1218 - acc: 0.9567\n",
      "Epoch 82/100\n",
      "106080/106080 [==============================] - 45s 428us/sample - loss: 0.1206 - acc: 0.9574\n",
      "Epoch 83/100\n",
      "106080/106080 [==============================] - 45s 420us/sample - loss: 0.1205 - acc: 0.9578\n",
      "Epoch 84/100\n",
      "106080/106080 [==============================] - 44s 418us/sample - loss: 0.1205 - acc: 0.9571\n",
      "Epoch 85/100\n",
      "106080/106080 [==============================] - 42s 395us/sample - loss: 0.1233 - acc: 0.9561\n",
      "Epoch 86/100\n",
      "106080/106080 [==============================] - 48s 456us/sample - loss: 0.1209 - acc: 0.9569\n",
      "Epoch 87/100\n",
      "106080/106080 [==============================] - 47s 448us/sample - loss: 0.1179 - acc: 0.9582\n",
      "Epoch 88/100\n",
      "106080/106080 [==============================] - 45s 424us/sample - loss: 0.1147 - acc: 0.9588\n",
      "Epoch 89/100\n",
      "106080/106080 [==============================] - 46s 435us/sample - loss: 0.1158 - acc: 0.9586\n",
      "Epoch 90/100\n",
      "106080/106080 [==============================] - 44s 417us/sample - loss: 0.1146 - acc: 0.9594\n",
      "Epoch 91/100\n",
      "106080/106080 [==============================] - 45s 426us/sample - loss: 0.1153 - acc: 0.9586\n",
      "Epoch 92/100\n",
      "106080/106080 [==============================] - 45s 427us/sample - loss: 0.1140 - acc: 0.9590\n",
      "Epoch 93/100\n",
      "106080/106080 [==============================] - 46s 436us/sample - loss: 0.1143 - acc: 0.9585\n",
      "Epoch 94/100\n",
      "106080/106080 [==============================] - 46s 433us/sample - loss: 0.1097 - acc: 0.9609\n",
      "Epoch 95/100\n",
      "106080/106080 [==============================] - 46s 431us/sample - loss: 0.1091 - acc: 0.9607\n",
      "Epoch 96/100\n",
      "106080/106080 [==============================] - 46s 430us/sample - loss: 0.1132 - acc: 0.9586\n",
      "Epoch 97/100\n",
      "106080/106080 [==============================] - 46s 429us/sample - loss: 0.1091 - acc: 0.9614\n",
      "Epoch 98/100\n",
      "106080/106080 [==============================] - 45s 423us/sample - loss: 0.1067 - acc: 0.9618\n",
      "Epoch 99/100\n",
      "106080/106080 [==============================] - 46s 432us/sample - loss: 0.1050 - acc: 0.9624\n",
      "Epoch 100/100\n",
      "106080/106080 [==============================] - 46s 430us/sample - loss: 0.1061 - acc: 0.9617\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcda5d00ed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential() #Initiates the model\n",
    "model.add(Dense(Ni, input_dim=img.shape[1], activation=activation)) #This is our input layer, has the same number of nodes as dimensions in our data\n",
    "model.add(Dense(layer_size, activation=activation)) # This is the hidden layers\n",
    "model.add(Dense(No, activation='softmax'))#Output layer. We have 26 letters, so output layers should have 26 units. Softmax is used for multi-class classification\n",
    "                                                            \n",
    "optimizer = Adam(lr=learning_rate, beta_1=beta[0], beta_2=beta[1])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = [\"accuracy\"]) #Accuracy and categorical cross-entropy is used as it's a classification problem\n",
    "model.fit(X_train_val, y_train_val, epochs=100,  batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = np.argmax(model.predict(X_test), axis=-1)\n",
    "acc_test = round(accuracy_score(y_test, y_pred_test), 3)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87360/87360 [==============================] - 10s 119us/sample - loss: 0.0952 - acc: 0.9666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09524973370544203, 0.96658653]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
